{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anish-mulay07/Reinforcement-Learning/blob/main/Options.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn7PKu9r0asK"
      },
      "source": [
        "# Tutorial 8 - Options\n",
        "\n",
        "Please complete this tutorial to get an overview of options and an implementation of SMDP Q-Learning and Intra-Option Q-Learning.\n",
        "\n",
        "\n",
        "### References:\n",
        "\n",
        " [Recent Advances in Hierarchical Reinforcement\n",
        "Learning](https://people.cs.umass.edu/~mahadeva/papers/hrl.pdf) is a strong recommendation for topics in HRL that was covered in class. Watch Prof. Ravi's lectures on moodle or nptel for further understanding the core concepts. Contact the TAs for further resources if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_DODRgW_ZKS"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "A bunch of imports, you don't have to worry about these\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import gym\n",
        "# from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYNA5kiH_esJ",
        "outputId": "23dfedb5-c695-4c12-da0e-2771b6a87fc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36\n",
            "Number of states: 48\n",
            "Number of actions that an agent can take: 4\n",
            "Action taken: left\n",
            "Transition probability: False\n",
            "Next state: 36\n",
            "Reward recieved: -1\n",
            "Terminal state: False\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "The environment used here is extremely similar to the openai gym ones.\n",
        "At first glance it might look slightly different.\n",
        "The usual commands we use for our experiments are added to this cell to aid you\n",
        "work using this environment.\n",
        "'''\n",
        "\n",
        "#Setting up the environment\n",
        "from gym.envs.toy_text.cliffwalking import CliffWalkingEnv\n",
        "env = CliffWalkingEnv()\n",
        "\n",
        "env.reset()\n",
        "\n",
        "#Current State\n",
        "print(env.s)\n",
        "\n",
        "# 4x12 grid = 48 states\n",
        "print (\"Number of states:\", env.nS)\n",
        "\n",
        "# Primitive Actions\n",
        "action = [\"up\", \"right\", \"down\", \"left\"]\n",
        "#correspond to [0,1,2,3] that's actually passed to the environment\n",
        "\n",
        "# either go left, up, down or right\n",
        "print (\"Number of actions that an agent can take:\", env.nA)\n",
        "\n",
        "# Example Transitions\n",
        "rnd_action = random.randint(0, 3)\n",
        "print (\"Action taken:\", action[rnd_action])\n",
        "next_state, reward, is_terminal, t_prob, _ = env.step(rnd_action)\n",
        "print (\"Transition probability:\", t_prob)\n",
        "print (\"Next state:\", next_state)\n",
        "print (\"Reward recieved:\", reward)\n",
        "print (\"Terminal state:\", is_terminal)\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apuaOxavDXus"
      },
      "source": [
        "#### Options\n",
        "We custom define very simple options here. They might not be the logical options for this settings deliberately chosen to visualise the Q Table better.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4MRC1p2DZbp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "6993bcdc-d712-4b28-e135-4e2b6143782e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNow the new action space will contain\\nPrimitive Actions: [\"up\", \"right\", \"down\", \"left\"]\\nOptions: [\"Away\",\"Close\"]\\nTotal Actions :[\"up\", \"right\", \"down\", \"left\", \"Away\", \"Close\"]\\nCorresponding to [0,1,2,3,4,5]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# We are defining two more options here\n",
        "# Option 1 [\"Away\"] - > Away from Cliff (ie keep going up)\n",
        "# Option 2 [\"Close\"] - > Close to Cliff (ie keep going down)\n",
        "\n",
        "def Away(env,state):\n",
        "\n",
        "    optdone = False\n",
        "    optact = 0\n",
        "\n",
        "    if (int(state/12) == 0):\n",
        "        optdone = True\n",
        "\n",
        "    return [optact,optdone]\n",
        "\n",
        "def Close(env,state):\n",
        "\n",
        "    optdone = False\n",
        "    optact = 2\n",
        "\n",
        "    if (int(state/12) == 2):\n",
        "        optdone = True\n",
        "\n",
        "    return [optact,optdone]\n",
        "\n",
        "\n",
        "'''\n",
        "Now the new action space will contain\n",
        "Primitive Actions: [\"up\", \"right\", \"down\", \"left\"]\n",
        "Options: [\"Away\",\"Close\"]\n",
        "Total Actions :[\"up\", \"right\", \"down\", \"left\", \"Away\", \"Close\"]\n",
        "Corresponding to [0,1,2,3,4,5]\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmv5c0XoK8GA"
      },
      "source": [
        "# Task 1\n",
        "Complete the code cell below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bh_oghc7Ledh"
      },
      "outputs": [],
      "source": [
        "#Q-Table: (States x Actions) === (env.ns(48) x total actions(6))\n",
        "q_values_SMDP = np.zeros((48,6))\n",
        "q_values_IOQL = np.zeros((48, 6))\n",
        "\n",
        "#Update_Frequency Data structure? Check TODO 4\n",
        "update_frequency_smdp = np.zeros((48, 6))  # Same shape as q_values_SMDP\n",
        "update_frequency_IOQL = np.zeros((48, 6))\n",
        "\n",
        "# TODO: epsilon-greedy action selection function\n",
        "def egreedy_policy(q_values,state,epsilon):\n",
        "  if random.uniform(0, 1) < epsilon:\n",
        "        # Exploration: choose a random action\n",
        "        action = np.random.choice(env.nA + 2)  # +2 to account for the options \"Away\" and \"Close\"\n",
        "  else:\n",
        "        # Exploitation: choose the action with the highest Q-value\n",
        "        action = np.argmax(q_values[state])\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8VJYkqoLqlO"
      },
      "source": [
        "# Task 2\n",
        "Below is an incomplete code cell with the flow of SMDP Q-Learning. Complete the cell and train the agent using SMDP Q-Learning algorithm.\n",
        "Keep the **final Q-table** and **Update Frequency** table handy (You'll need it in TODO 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "b73V3VifdcXk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ok_5eQM7OCTj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "264555bf-fe4a-4bea-a337-bca854335f9f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-860a8ee72184>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;31m# Execute the Close option\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0moptact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;31m# Update reward_bar using the cumulative reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/toy_text/cliffwalking.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mtransitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategorical_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/toy_text/utils.py\u001b[0m in \u001b[0;36mcategorical_sample\u001b[0;34m(prob_n, np_random)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"\"\"Sample from categorical distribution where each row specifies class probabilities.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprob_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcsprob_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsprob_n\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_cumsum_dispatcher\u001b[0;34m(a, axis, dtype, out)\u001b[0m\n\u001b[1;32m   2506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2508\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_cumsum_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2509\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#### SMDP Q-Learning\n",
        "\n",
        "# Add parameters you might need here\n",
        "gamma = 0.9\n",
        "alpha = 0.9\n",
        "total_rewards = 0\n",
        "\n",
        "# Iterate over 1000 episodes\n",
        "for _ in tqdm(range(1000)):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    # While episode is not over\n",
        "    while not done:\n",
        "\n",
        "        # Choose action\n",
        "        action = egreedy_policy(q_values_SMDP, state, epsilon=0.1)\n",
        "\n",
        "        # Checking if primitive action\n",
        "        if action < 4:\n",
        "            # Perform regular Q-Learning update for state-action pair\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            max_next_q_value = np.max(q_values_SMDP[next_state])\n",
        "            q_values_SMDP[state][action] += alpha * (reward + gamma * max_next_q_value - q_values_SMDP[state][action])\n",
        "\n",
        "            # Update the update frequency\n",
        "            update_frequency_smdp[state][action] += 1\n",
        "\n",
        "\n",
        "        # Checking if action chosen is an option\n",
        "        reward_bar = 0\n",
        "        if action == 4: # action => Away option\n",
        "\n",
        "            optdone = False\n",
        "            while (optdone == False):\n",
        "\n",
        "                # Think about what this function might do?\n",
        "                optact,optdone = Away(env,state)\n",
        "                next_state, reward, done,_,_ = env.step(optact)\n",
        "\n",
        "                # Is this formulation right? What is this term?\n",
        "                reward_bar = gamma*reward_bar + reward\n",
        "\n",
        "                # Complete SMDP Q-Learning Update\n",
        "                # Remember SMDP Updates. When & What do you update?\n",
        "                q_values_SMDP[state][action] += alpha * (reward_bar - q_values_SMDP[state][action])\n",
        "\n",
        "                # Update the update frequency\n",
        "                update_frequency_smdp[state][action] += 1\n",
        "\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "        if action == 5: # action => Close option\n",
        "          optdone = False\n",
        "          while not optdone:\n",
        "                # Execute the Close option\n",
        "                optact, optdone = Close(env, state)\n",
        "                next_state, reward, done, _, _ = env.step(optact)\n",
        "\n",
        "                # Update reward_bar using the cumulative reward\n",
        "                reward_bar = gamma * reward_bar + reward\n",
        "\n",
        "                # Complete SMDP Q-Learning Update for the Close option\n",
        "                q_values_SMDP[state][action] += alpha * (reward_bar - q_values_SMDP[state][action])\n",
        "\n",
        "                # Update the update frequency\n",
        "                update_frequency_smdp[state][action] += 1\n",
        "\n",
        "                state = next_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SQFbRCHWQyO"
      },
      "source": [
        "# Task 3\n",
        "Using the same options and the SMDP code, implement Intra Option Q-Learning (In the code cell below). You *might not* always have to search through options to find the options with similar policies, think about it. Keep the **final Q-table** and **Update Frequency** table handy (You'll need it in TODO 4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6A2TdUHWVUN"
      },
      "outputs": [],
      "source": [
        "#### Intra-Option Q-Learning\n",
        "\n",
        "#\n",
        "\n",
        "#\n",
        "\n",
        "#\n",
        "\n",
        "#\n",
        "\n",
        "#\n",
        "\n",
        "# Add parameters you might need here\n",
        "gamma = 0.9\n",
        "alpha = 0.1  # Learning rate\n",
        "\n",
        "# Iterate over 1000 episodes\n",
        "for _ in range(1000):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    # While episode is not over\n",
        "    while not done:\n",
        "\n",
        "        # Choose action\n",
        "        action = egreedy_policy(q_values_IOQL, state, epsilon=0.1)\n",
        "\n",
        "        # Checking if primitive action\n",
        "        if action < 4:\n",
        "            # Perform regular Q-Learning update for state-action pair\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            max_next_q_value = np.max(q_values_IOQL[next_state])\n",
        "            q_values_IOQL[state][action] += alpha * (reward + gamma * max_next_q_value - q_values_IOQL[state][action])\n",
        "\n",
        "            # Update the update frequency\n",
        "            update_frequency_IOQL[state][action] += 1\n",
        "\n",
        "        # Checking if action chosen is an option\n",
        "        reward_bar = 0\n",
        "        if action == 4: # action => Away option\n",
        "\n",
        "            optdone = False\n",
        "            while not optdone:\n",
        "                # Execute the Away option\n",
        "                optact, optdone = Away(env, state)\n",
        "                next_state, reward, done, _ = env.step(optact)\n",
        "\n",
        "                # Update reward_bar using the cumulative reward\n",
        "                reward_bar = gamma * reward_bar + reward\n",
        "\n",
        "                # Complete Intra Option Q-Learning Update for the Away option\n",
        "                q_values_IOQL[state][action] += alpha * (reward + gamma * np.max(q_values_IOQL[next_state]) - q_values_IOQL[state][action])\n",
        "\n",
        "                # Update the update frequency\n",
        "                update_frequency_IOQL[state][action] += 1\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "        if action == 5: # action => Close option\n",
        "            optdone = False\n",
        "            while not optdone:\n",
        "                # Execute the Close option\n",
        "                optact, optdone = Close(env, state)\n",
        "                next_state, reward, done, _ = env.step(optact)\n",
        "\n",
        "                # Update reward_bar using the cumulative reward\n",
        "                reward_bar = gamma * reward_bar + reward\n",
        "\n",
        "                # Complete Intra Option Q-Learning Update for the Close option\n",
        "                q_values_IOQL[state][action] += alpha * (reward + gamma * np.max(q_values_IOQL[next_state]) - q_values_IOQL[state][action])\n",
        "\n",
        "                # Update the update frequency\n",
        "                update_frequency_IOQL[state][action] += 1\n",
        "\n",
        "                state = next_state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzUgcwL-VfkO"
      },
      "source": [
        "# Task 4\n",
        "Compare the two Q-Tables and Update Frequencies and provide comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8mZE74_Vhmg"
      },
      "outputs": [],
      "source": [
        "# Use this cell for Task 4 Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SemE13ORV04n"
      },
      "source": [
        "Use this text cell for your comments - Task 4\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5LBh6_lOVBdN"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}